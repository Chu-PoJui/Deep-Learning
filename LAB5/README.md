# Lab 5 â€” Value-Based Reinforcement Learning  
Deep Learning @ NYCU (Spring 2025, TAICA)

This project implements and compares **vanilla DQN** and several **enhanced DQN variants** on classic control and Atari environments. The goal is to study how Double DQN, Prioritized Experience Replay (PER), Multi-Step Return, and Dueling Networks affect **sample efficiency** and **final performance**.

---

## ğŸ“Œ Overview
- **Task 1**: Vanilla DQN on **CartPole-v1**  
- **Task 2**: Vanilla DQN on **Pong-v5** (visual input)  
- **Task 3**: Enhanced DQN on **Pong-v5**  
  - Double DQN  
  - PER + Multi-step return (n=5)  
  - Dueling DQN architecture  
  - Cosine learning rate scheduling  

---

## âš™ï¸ Environment

| Package       | Version   |
|---------------|-----------|
| Python        | â‰¥ 3.8     |
| torch         | â‰¥ 2.0.0   |
| gymnasium     | 1.1.1     |
| ale-py        | â‰¥ 0.10.0  |
| opencv-python | latest    |
| wandb         | latest    |

---

## ğŸ“‚ File Structure

```
LAB5_11028141_POJUI/
â”œâ”€ report.pdf # Technical report
â”œâ”€ demo.mp4 # 5â€“6 min demo video
â”œâ”€ code/
â”‚ â”œâ”€ dqn.py # Main implementation
â”‚ â”œâ”€ task1_cartpole.py # Training CartPole
â”‚ â”œâ”€ task2_pong.py # Training Pong
â”‚ â”œâ”€ task3_enhanced.py # Enhanced DQN
â”‚ â”œâ”€ test_model_task1.py # Evaluation script (CartPole)
â”‚ â”œâ”€ test_model_task2.py # Evaluation script (Pong vanilla)
â”‚ â””â”€ test_model_task3.py # Evaluation script (Pong enhanced)
â”œâ”€ task1_cartpole.pt # Best CartPole model
â”œâ”€ task2_pong.pt # Best Pong vanilla model
â”œâ”€ task3_pong200k.pt # Enhanced DQN snapshot @200k
â”œâ”€ task3_pong400k.pt
â”œâ”€ task3_pong600k.pt
â”œâ”€ task3_pong800k.pt
â””â”€ task3_pong1M.pt
```


---

## ğŸš€ How to Run

### Task 1 â€” CartPole (Vanilla DQN)
```bash
python code/task1_cartpole.py --episodes 3000
python code/test_model_task1.py --model task1_cartpole.pt --eps 20 --render
```


### Task 2 â€” Pong (Vanilla DQN)
```bash
python code/task2_pong.py --steps 6500000
python code/test_model_task2.py --model task2_pong.pt --episodes 10 --output eval_videos
```


### Task 3 â€” Pong (Enhanced DQN)
```bash
python code/task3_enhanced.py --steps 1000000
python code/test_model_task3.py --model task3_pong800k.pt --episodes 10 --output eval_videos_task3
```

---

## ğŸ“Š Results

- **Task 1 (CartPole-v1)**: solved in ~30k steps, stable avg reward = 500  
- **Task 2 (Pong-v5, Vanilla)**: required ~6.5M steps to reach avg score 19  
- **Task 3 (Pong-v5, Enhanced)**: reached avg score 19 within ~735k steps (~8Ã— faster)  

### Sample Efficiency Comparison

| Method                  | Steps to â‰¥19 | Relative |
|--------------------------|--------------|----------|
| Vanilla DQN (Task 2)    | ~6.5M        | 1Ã—       |
| Enhanced DQN (Task 3)   | ~0.735M      | â†“ ~89%   |

---

## ğŸ¥ Demo  
- [Demo Video (5â€“6 min)](./LAB5.mp4)  
  Includes code walkthrough and evaluation runs for Tasks 1â€“3.  

---

## ğŸ“‘ References
- Mnih et al., *Human-level control through deep reinforcement learning*, Nature 2015  
- Hessel et al., *Rainbow: Combining improvements in deep RL*, AAAI 2018  
- Schaul et al., *Prioritized Experience Replay*, ICLR 2016  

